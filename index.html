<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Itamar Hagay Pres</title>

  <meta name="author" content="Itamar Hagay Pres">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Itamar Hagay Pres</name>
                  </p>

                  <!-- Description -->
                  <p>I am an undergraduate at the University of Michigan studying Computer Science with a minor in
                    Mathematics. My primary
                    focus is on making artificial intelligence systems safer, more interpretable, and better
                    aligned with human values. More sepcifically, I aim to explore how large language models (LLMs) can
                    operate ethically
                    and effectively in diverse and dynamic contexts.

                  </p>
          

                  <p>At Michigan, I am involved with the <a href="https://lit.eecs.umich.edu/">Language and Information Technologies Lab</a>
                  and have worked alongside <a href="https://ajyl.github.io/about">Dr. Andrew Lee</a> and <a
                    href="https://web.eecs.umich.edu/~mihalcea/">Prof. Rada Mihaclea</a> where I have leveraged interpretibility to study
                  toxicity in LLMs.
                  </p>

                  <p>
                    Since this past summer, I have been interning at the <a href="https://www.kasl.ai/">Krueger AI Safety Lab</a> at the
                    University of Cambridge working alongside <a href="https://davidscottkrueger.com/">Prof. David Krueger</a>, <a
                      href="https://ekdeepslubana.github.io/">Dr. Ekdeep Singh Lubana</a>, and <a
                      href="https://www.linkedin.com/in/laura-ruis-333aaa73/">Laura Ruis</a> to develop new inference-time methods for model
                    behavioral control.
                    
                  </p>

                  <p>
                    Most recently, I have also been working with <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a> at
                    the <a href="https://cbs.fas.harvard.edu/">Harvard Center for Brain Science</a> to mechanistically study in-context
                    learning.
                  </p>


                  <p>
                    I first started doing interpretibility research with <a href="https://www.neelnanda.io/about">Neel Nanda</a> through the <a
                      href="https://www.matsprogram.org/">Machine Learning Alignment and Thoery Scholors Program</a>.
                  
                  
                  </p>


                  <!-- Stuff -->
                  <p style="text-align:center">
                    <a href="mailto:presi@umich.edu">Email</a> &nbsp/&nbsp
                    <a href="data/resume.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=FuETqOUAAAAJ&hl=en">Google Scholar</a>
                  </p>
                </td>

                <!-- Front image -->
                <td style="padding:2.5%;width:40%;max-width:40%;text-align: left">
                  <a href="images/headshot.png"><img style="width:100%;max-width:100%;display: block" alt="profile photo"
                      src="images/headshot.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>



          <!-- Publications -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications (* denotes equal contribution)</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/toxicity.png" alt="Toxicity DPO" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2401.01967">
                    <papertitle>A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity</papertitle>
                  </a>
                  <br>
                  <a>Andrew Lee</a>,
                  <a>Xiaoyan Bai</a>,
                  <strong>Itamar Pres</strong>,
                  <a>Martin Wattenberg</a>,
                  <a>Jonathan K. Kummerfeld</a>,
                  and 
                  <a>Rada Mihalcea</a>
                  <br>
                  <em>ICML</em>, 2024 <font color="red"> (Oral)</font>
                  <br>


                  <p>
                    We study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces
                    toxicity. We first study how toxicity is represented and elicited in pre-trained language
                    models (GPT2-medium, Llama2-7b). We then apply DPO to reduce toxicity and find that capabilities learned from pre-training are not removed,
                    but rather bypassed. We use this insight to demonstrate a simple method to un-align the models,
                    reverting them back to their toxic behavior.

                  </p>
                </td>
              </tr>


          



              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        Website template source available <a
                          href="https://github.com/jonbarron/jonbarron_website">here</a>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>